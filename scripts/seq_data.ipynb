{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from utils import load_data\n",
    "from scipy.sparse import coo_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load browsing done...\n",
      "load search done...\n",
      "load sku done...\n",
      "load info done...\n"
     ]
    }
   ],
   "source": [
    "browsing, search, sku, info = load_data('../dataset/new/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['session_id_hash', 'query_vector', 'clicked_skus_hash',\n",
       "       'product_skus_hash', 'server_timestamp_epoch_ms'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'detail', 1: 'pageview', 2: 'add', 3: 'purchase', 4: 'remove'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.idx2act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "brwosing = browsing[browsing['product_action'] != 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2item = browsing.dropna().drop_duplicates(subset=['hashed_url', 'product_sku_hash']).groupby('hashed_url')['product_sku_hash'].agg(list)\n",
    "url2item = url2item.reset_index()\n",
    "url2item['product_sku_hash'] = url2item['product_sku_hash'].apply(lambda x: x[0])\n",
    "url2item = np.vstack(url2item.values).T\n",
    "url2item = url2item.astype(int).tolist()\n",
    "url2item = dict(zip(*url2item))\n",
    "browsing['product_sku_hash'] = browsing['hashed_url'].map(url2item.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add click data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_click = search.explode(\"clicked_skus_hash\")\n",
    "# search_click = search_click.dropna(subset=['clicked_skus_hash'])\n",
    "# search['item_count'] = search.groupby('session_id_hash')['clicked_skus_hash'].transform('count')\n",
    "# search = search.sort_values(by=['session_id_hash', 'server_timestamp_epoch_ms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_view = search.explode(\"product_skus_hash\")\n",
    "# search_view = search_view.dropna(subset=['product_skus_hash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_click = search_click[['session_id_hash', 'clicked_skus_hash', 'server_timestamp_epoch_ms']]\n",
    "# search_click.columns = ['sess_id', 'sku_id', 'timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_view = search_view[['session_id_hash', 'product_skus_hash', 'server_timestamp_epoch_ms']]\n",
    "# search_view.columns = ['sess_id', 'sku_id', 'timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search = pd.concat([search_click, search_view])\n",
    "# search.drop_duplicates(subset=['sess_id', 'sku_id'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_train =  search.loc['train', :].copy()\n",
    "# search_test = search.loc['test', :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "browsing.dropna(subset=['product_sku_hash'], inplace=True)\n",
    "browsing.drop_duplicates(subset=['session_id_hash', 'product_sku_hash'], keep='last', inplace=True)\n",
    "# browsing = browsing.sort_values(by=['session_id_hash', 'server_timestamp_epoch_ms'])\n",
    "browsing['product_sku_hash'] = browsing['product_sku_hash'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "browsing = browsing[['session_id_hash', 'product_sku_hash', 'server_timestamp_epoch_ms']]\n",
    "browsing.columns = ['sess_id', 'sku_id', 'timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "browsing_train = browsing.loc['train', :].copy()\n",
    "browsing_test = browsing.loc['test', :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.concat([browsing_train, search_train])\n",
    "# test = pd.concat([browsing_test, search_test])\n",
    "train = browsing_train.copy()\n",
    "test = browsing_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop_duplicates(subset=['sess_id', 'sku_id'], keep='last', inplace=True)\n",
    "train.drop_duplicates(subset=['sess_id', 'sku_id'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['item_count'] = train.groupby('sess_id')['sku_id'].transform('count')\n",
    "train = train[train['item_count'] >=2]\n",
    "train.drop(columns=['item_count'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test['item_count'] = test.groupby('sess_id')['sku_id'].transform('count')\n",
    "# test = test[test['item_count'] >=2]\n",
    "# test.drop(columns=['item_count'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pad = test.groupby('sess_id').tail(1).copy()\n",
    "test_pad['timestamp'] += 1\n",
    "test_dev = pd.concat([test, test_pad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dev = test_dev.sort_values(by=['sess_id', 'timestamp'])\n",
    "train = train.sort_values(by=['sess_id', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = []\n",
    "valid_set = []\n",
    "test_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 5\n",
    "for sess_id, item_ids in train.groupby('sess_id')['sku_id']:\n",
    "    item_ids = item_ids.tolist()\n",
    "    for idx in range(1, len(item_ids)-1):\n",
    "        if idx < seq_len:\n",
    "            seq = item_ids[:idx] + [-1] * (seq_len - idx)\n",
    "        else:\n",
    "            seq = item_ids[idx - seq_len:idx]\n",
    "        target = item_ids[idx]\n",
    "        train_set.append((sess_id, seq, target))\n",
    "        \n",
    "    idx = len(item_ids)-1\n",
    "    if idx < seq_len:\n",
    "        seq = item_ids[:idx] + [-1] * (seq_len - idx)\n",
    "    else:\n",
    "        seq = item_ids[idx - seq_len:idx]\n",
    "    target = item_ids[idx]\n",
    "    valid_set.append((sess_id, seq, target))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sess_id, item_ids in test_dev.groupby('sess_id')['sku_id']:\n",
    "    item_ids = item_ids.tolist()\n",
    "    for idx in range(1, len(item_ids)-1):\n",
    "        if idx < seq_len:\n",
    "            seq = item_ids[:idx] + [-1] * (seq_len - idx)\n",
    "        else:\n",
    "            seq = item_ids[idx - seq_len:idx]\n",
    "        target = item_ids[idx]\n",
    "        train_set.append((sess_id, seq, target))\n",
    "        \n",
    "    idx = len(item_ids)-1\n",
    "    if idx < seq_len:\n",
    "        seq = item_ids[:idx] + [-1] * (seq_len - idx)\n",
    "    else:\n",
    "        seq = item_ids[idx - seq_len:idx]\n",
    "    target = item_ids[idx]\n",
    "    test_set.append((sess_id, seq, target))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3395175, 1512420, 96108)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(valid_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, data in zip(['train', 'valid', 'test'], [train_set, valid_set, test_set]):\n",
    "    pathname = f'../dataset/prepared/{name}.csv'\n",
    "    with open(pathname, 'w') as f:\n",
    "        for user_id, seq, target in data:\n",
    "            f.write(','.join([str(user_id), '|'.join(map(str, seq)), str(target)]))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
